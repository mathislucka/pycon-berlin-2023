{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from haystack.nodes.prompt import PromptNode, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = os.environ.get(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What we'll build\n",
    "\n",
    "We will build our personal assistant using OpenAI's `text-davinci-003` model.\n",
    "\n",
    "We proceed step by step, only giving some code as a starter and then letting the model build up the whole system on its own.\n",
    "\n",
    "In the end, we'll walk away with a retrieval augmented text and code generation pipeline that can easily be customized to your own data.\n",
    "\n",
    "Let's start!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PromptNode\n",
    "\n",
    "At first, we'll initialize a `PromptNode` using the `text-davinci-003` model.\n",
    "\n",
    "The PromptNode is Haystack's abstraction around large language models (LLM).\n",
    "\n",
    "It provides a unified interface to many open source and commercial LLMs.\n",
    "\n",
    "Find the full `PromptNode` documentation here: [https://docs.haystack.deepset.ai/docs/prompt_node](https://docs.haystack.deepset.ai/docs/prompt_node)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"text-davinci-003\"\n",
    "\n",
    "# The prompt template manages how instructions (aka prompts) will be passed to the PromptNode\n",
    "# Via a placeholder syntax that is similar to python f-strings, it allows us to enrich the user's instructions with outputs\n",
    "# of other nodes that might be used with the PromptNode in a pipeline.\n",
    "# Here, we directly pass the user instruction (query) to the PromptNode without adding any other extra data.\n",
    "# See https://docs.haystack.deepset.ai/docs/prompt_node#prompttemplates for detailed documentation on prompt templates\n",
    "prompt_template = PromptTemplate(\"direct\", \"{query}\")\n",
    "\n",
    "\n",
    "prompt_node = PromptNode(model_name_or_path=model_name, api_key=openai_key, max_length=700, default_prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting some initial context\n",
    "\n",
    "As a first step, I've copied the documentation for our `Crawler` component.\n",
    "\n",
    "Given a list of URLs, the `Crawler` goes and scrapes text from them.\n",
    "\n",
    "We can then transform the text into `Document`s and use a `Pipeline` to store these documents in a `DocumentStore`.\n",
    "\n",
    "A `DocumentStore` makes these documents accessible for later use. You can also embed the documents as vectors before writing them to the `DocumentStore`.\n",
    "\n",
    "We have a range of `DocumentStore`s to choose from, some of them are:\n",
    "\n",
    "- `FAISSDocumentStore`\n",
    "- `WeaviateDocumentStore`\n",
    "- `QdrantDocumentStore`\n",
    "- `ElasticsearchDocumentStore`\n",
    "\n",
    "We will use a simple `InMemoryDocumentStore` for this demo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_help = \"\"\"\n",
    "Crawler\n",
    "The Crawler scrapes the text from a website and creates a Document object out of it. For example, you can use the Crawler to turn the contents of a website into Documents to use for search.\n",
    "\n",
    "Suggest Edits\n",
    "Position in a Pipeline\tAt the very beginning of an indexing Pipeline\n",
    "Input\tFiles\n",
    "Output\tDocuments\n",
    "Classes\tCrawler\n",
    "Usage\n",
    "To use a Crawler on its own, run:\n",
    "\n",
    "Python\n",
    "\n",
    "from haystack.nodes import Crawler\n",
    "\n",
    "crawler = Crawler(output_dir=\"crawled_files\") # This tells the Crawler where to store the crawled files\n",
    "docs = crawler.crawl(\n",
    "    urls=[\"https://haystack.deepset.ai/docs/get-started\"], # This tells the Crawler which URLs to crawl\n",
    "    filter_urls=[\"haystack\"], # Here, you can pass regular expressions that the crawled URLs must comply with\n",
    "    crawler_depth=1 # This tells the Crawler to follow only the links that it finds on the initial URLs\n",
    ")\n",
    "Example Script\n",
    "This script shows you how to use a Crawler in a pipeline.\n",
    "\n",
    "Python\n",
    "\n",
    "################################################################################\n",
    "#                                                                              #\n",
    "#             An Example of a Pipeline Using Crawler                           #\n",
    "#                                                                              #\n",
    "#  NOTE: You need a running Elasticsearch container for this to work.          #\n",
    "#  If you don't have one, exchange ElasticsearchDocumentStore for another      #\n",
    "#  document store, like SQLDocumentStore or InMemoryDocumentStore. Bear in     #\n",
    "#  mind though that the code wasn't tested on them and you might encounter      #\n",
    "#  errors.                                                                  #\n",
    "#                                                                              #\n",
    "################################################################################\n",
    "\n",
    "from haystack.pipelines import Pipeline\n",
    "from haystack.nodes import Crawler, PreProcessor, BM25Retriever, FARMReader\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "\n",
    "# Create the document store. You need it to:\n",
    "#  1. Store the documents you crawled and preprocessed (with an indexing pipeline).\n",
    "#  2. Extract the documents that contain the answer to your question (with a query pipeline).\n",
    "#     document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "\n",
    "\n",
    "#\n",
    "# Step 1: Get the data, clean it, and store it.\n",
    "#\n",
    "\n",
    "# NOTE: Run this code just once, every time you create a new Elasticsearch container. Comment it out afterwards.\n",
    "\n",
    "# Let's create the indexing pipeline. It will contain:\n",
    "#  1. A Crawler node that fetches text from a website.\n",
    "#  2. A PreProcessor that makes the documents friendly to the Retriever\n",
    "#  3. The DocumentStore that receives the documents and stores them.\n",
    "\n",
    "crawler = Crawler(\n",
    "    urls=[\"https://haystack.deepset.ai\"],   # Websites to crawl\n",
    "    crawler_depth=1,    # How many links to follow\n",
    "    output_dir=\"crawled_files\",  # The directory to store the crawled files, not very important, we don't use the files in this example\n",
    ")\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=500,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_node(component=crawler, name=\"crawler\", inputs=['File'])\n",
    "indexing_pipeline.add_node(component=preprocessor, name=\"preprocessor\", inputs=['crawler'])\n",
    "indexing_pipeline.add_node(component=document_store, name=\"document_store\", inputs=['preprocessor'])\n",
    "\n",
    "indexing_pipeline.run()\n",
    "\n",
    "\n",
    "#\n",
    "# Step 2: Use the data to answer questions.\n",
    "#\n",
    "\n",
    "# NOTE: You can run this code as many times as you like.\n",
    "\n",
    "# Let's create a query pipeline. It will contain:\n",
    "#  1. A Retriever that gets the relevant documents from the DocumentStore.\n",
    "#  2. A Reader that locates the answers inside the documents.\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "reader =  FARMReader(model_name_or_path=\"deepset/roberta-base-squad2-distilled\")\n",
    "\n",
    "query_pipeline = Pipeline()\n",
    "query_pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "query_pipeline.add_node(component=reader, name=\"reader\", inputs=[\"retriever\"])\n",
    "\n",
    "results = query_pipeline.run(query=\"What can I use Haystack for?\")\n",
    "\n",
    "print(\"\\nQuestion: \", results[\"query\"])\n",
    "print(\"\\nAnswers:\")\n",
    "for answer in results[\"answers\"]:\n",
    "    print(\"- \", answer.answer)\n",
    "print(\"\\n\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://haystack.deepset.ai/tutorials/22_pipeline_with_promptnode',\n",
    "    'https://docs.haystack.deepset.ai/docs/pipelines',\n",
    "    'https://docs.haystack.deepset.ai/docs/ready_made_pipelines',\n",
    "    'https://docs.haystack.deepset.ai/docs/retriever',\n",
    "    'https://docs.haystack.deepset.ai/docs/agent',\n",
    "    'https://docs.haystack.deepset.ai/docs/intro',\n",
    "    'https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines',\n",
    "    'https://docs.haystack.deepset.ai/docs/prompt_node'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Our first instruction\n",
    "\n",
    "We will now prompt our model for the first time. We pass the crawler documentation as context and tell it to crawl additional Haystack documentation pages and to index them into a document store."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"Given this documentation: {crawler_help} ### Write a crawler node that crawls the following list of urls: {json.dumps(urls)}. Links should not be followed (depth 0). It should index these documents in an indexing pipeline. In the preprocessor, split it into 600 word chunks. The document store should use bm25. Do not use comments. Wrap the code in a code block like this: ```python\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = prompt_node.run(query=query)\n",
    "generated_code = out[\"results\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's inspect that generated code before we run it!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "```python\nfrom haystack.pipelines import Pipeline\nfrom haystack.nodes import Crawler, PreProcessor, BM25Retriever, FARMReader\nfrom haystack.document_stores import InMemoryDocumentStore\n\n\n# Create the document store. You need it to:\n#  1. Store the documents you crawled and preprocessed (with an indexing pipeline).\n#  2. Extract the documents that contain the answer to your question (with a query pipeline).\ndocument_store = InMemoryDocumentStore(use_bm25=True)\n\n\n# Let's create the indexing pipeline. It will contain:\n#  1. A Crawler node that fetches text from a website.\n#  2. A PreProcessor that makes the documents friendly to the Retriever\n#  3. The DocumentStore that receives the documents and stores them.\n\ncrawler = Crawler(\n    urls=[\"https://haystack.deepset.ai/tutorials/22_pipeline_with_promptnode\", \n          \"https://docs.haystack.deepset.ai/docs/pipelines\", \n          \"https://docs.haystack.deepset.ai/docs/ready_made_pipelines\", \n          \"https://docs.haystack.deepset.ai/docs/retriever\", \n          \"https://docs.haystack.deepset.ai/docs/agent\",\n          \"https://docs.haystack.deepset.ai/docs/intro\",\n          \"https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines\",\n          \"https://docs.haystack.deepset.ai/docs/prompt_node\"], \n    crawler_depth=0,\n    output_dir=\"crawled_files\",\n)\npreprocessor = PreProcessor(\n    clean_empty_lines=True,\n    clean_whitespace=True,\n    clean_header_footer=False,\n    split_by=\"word\",\n    split_length=600,\n    split_respect_sentence_boundary=True,\n)\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_node(component=crawler, name=\"crawler\", inputs=['File'])\nindexing_pipeline.add_node(component=preprocessor, name=\"preprocessor\", inputs=['crawler'])\nindexing_pipeline.add_node(component=document_store, name=\"document_store\", inputs=['preprocessor'])\n\nindexing_pipeline.run()\n```"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code looks great!\n",
    "\n",
    "`text-davinci-003` executed perfectly on our instructions, we can run the code to crawls these additional pages from the documentation page and index all of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 112.0.5615\n",
      "Get LATEST chromedriver version for 112.0.5615 google-chrome\n",
      "Driver [/Users/mathislucka/.wdm/drivers/chromedriver/mac64/112.0.5615.49/chromedriver] found in cache\n"
     ]
    },
    {
     "data": {
      "text/plain": "Preprocessing:   0%|          | 0/8 [00:00<?, ?docs/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afb34604ffdd4e6c82c12b7cf4146d81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document f7e9043805e9db17fda21984ff562d8a is 11133 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document d6452a2a65003120e9fc6a157969b788 is 10870 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document f32960edaf96d027fc02be5ca7a9bb61 is 10281 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 355e687bb55d3a8b5e316dc71e2ae5 is 10050 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Updating BM25 representation...:   0%|          | 0/32 [00:00<?, ? docs/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da9cc6bba1ae4a34a8488cd9e864f592"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exec(generated_code[9:-3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store, top_k=2)\n",
    "results = retriever.retrieve(query=\"How to use the PromptNode in a Pipeline?\")\n",
    "helper = results[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"Using this snippet from the documentation: {helper} ### Initialize a pipeline 'pipe_v2' with your own PromptTemplate 'template' and PromptNode 'pn'. The template will answer questions about technical documentation using the provided documents as context. Include the following in the template:\\n- questions should be answered in the style of technical documentation\\n- example code snippets should be provided if needed\\n- code should be formatted as ```python <code> ```\\n\\n. Make sure none of the instructions are missing.\\n\\nUse the existing retriever named `retriever` in the pipeline. Initialize the PromptNode with the 'gpt-3.5-turbo' model. The `api_key` for the PromptNode is stored in `openai_key`. PromptNode should be initialized with a max_length of 700. ```python\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = prompt_node.run(query=query)\n",
    "generated_code = out['results'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "```python\n\nfrom haystack.nodes import PromptNode, PromptTemplate\n\n# Define the template\ntemplate = PromptTemplate(\n    name=\"technical_doc_prompt\",\n    prompt_text=\"\"\"\nAnswer the given question using the provided context. Your answer should be in the style of technical documentation and should provide example code snippets if necessary. Format code as ```python <code> ```\n\nContext: {join(documents)}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\",\n)\n\n# Initialize the PromptNode \npn = PromptNode(model_name_or_path=\"gpt-3.5-turbo\", default_prompt_template=template, api_key=openai_key, max_length=700)\n\n# Initialize the pipeline\npipe_v2 = Pipeline()\n\n# Add the retriever\npipe_v2.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n\n# Add the PromptNode\npipe_v2.add_node(component=pn, name=\"prompt_node\", inputs=[\"retriever\"])\n```"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"```python\\n\\n\" + generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "exec(generated_code[:-3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Haystack is a Python library that provides a modular and extensible interface for building end-to-end search pipelines. It includes components for document retrieval, question answering, summarization, and more. The library also provides a PromptNode class, which allows you to easily integrate natural language prompts into your pipeline and use various language models for generating answers. The library is designed to be flexible and customizable, so you can easily configure it to suit your specific use case. \n\nExample code snippets for using PromptNode and creating custom prompts are provided in the context section above."
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pipe_v2.run(query=\"What is haystack?\")\n",
    "\n",
    "generated_docs = res[\"results\"][0]\n",
    "\n",
    "md(generated_docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipe_v2.run(query=\"How do I use the prompt node with open source models?\")\n",
    "\n",
    "generated_docs = res[\"results\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "To use the PromptNode with open source models, you can specify the model's name when initializing the PromptModel or PromptNode. For example, to use the flan t5 base model, you can initialize PromptNode like this:\n\n```python\nfrom haystack.nodes import PromptModel, PromptNode\n\nprompt_model = PromptModel(model_name_or_path=\"google/flan-t5-base\")\nprompt_node = PromptNode(prompt_model)\n```\n\nYou can replace \"google/flan-t5-base\" with the name of any open source model that you want to use. You can also specify an API key if necessary. For example, if you are using Hugging Face's API for models, you can initialize the model like this:\n\n```python\nprompt_model = PromptModel(model_name_or_path=\"model_name\", api_key=\"your_api_key\")\n```"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(generated_docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
